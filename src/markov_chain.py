"""Module for the MarkovChain class."""
import numpy as np


class MarkovChain:
    """
    MarkovChain class.

    The class provides the Markov-chain function (as run()).
    """

    def __init__(self, integrator_instance, potential_instance, kinetic_energy_instance, initial_step_size=1,
                 max_number_of_integration_steps=10, number_of_equilibration_iterations=100,
                 number_of_observations=1100, use_metropolis_accept_reject=True,
                 random_number_of_integration_steps=False):
        """
        The constructor of the MarkovChain class.

        Parameters
        ----------
        integrator_instance : Python class instance

        potential_instance : Python class instance

        kinetic_energy_instance : Python class instance

        initial_step_size : int, optional

        max_number_of_integration_steps : int, optional

        number_of_equilibration_iterations : int, optional

        number_of_observations : int, optional

        use_metropolis_accept_reject : Boolean, optional

        random_number_of_integration_steps : Boolean, optional

        Raises
        ------
        base.exceptions.ValueError
            If the prefactor equals 0.
        """
        if initial_step_size == 0:
            raise ValueError("Give a value not equal to 0 as the step size of the numerical integrator {0}.".format(
                self.__class__.__name__))
        if max_number_of_integration_steps == 0:
            raise ValueError("Give a value not equal to 0 as the number of numerical integration steps {0}.".format(
                self.__class__.__name__))
        if number_of_equilibration_iterations >= number_of_observations:
            raise ValueError("Set equilibration iterations less than the number of observations {0}.".format(
                self.__class__.__name__))
        if number_of_observations == 0:
            raise ValueError(
                "Give a value not equal to 0 as the number of observations of target distribution {0}.".format(
                    self.__class__.__name__))
        self._integrator_instance = integrator_instance
        self._potential_instance = potential_instance
        self._kinetic_energy_instance = kinetic_energy_instance
        self._step_size = initial_step_size
        self._max_number_of_integration_steps = max_number_of_integration_steps
        self._number_of_equilibration_iterations = number_of_equilibration_iterations
        self._number_of_observations = number_of_observations
        self._use_metropolis_accept_reject = use_metropolis_accept_reject
        self._random_number_of_integration_steps = random_number_of_integration_steps

    def run(self, support_variable, charges=None):
        """
        Runs the Markov chain and returns the generated observations of the target and momentum distributions.

        Parameters
        ----------
        support_variable : numpy_array
            For soft-matter models, one or many particle-particle separation vectors {r_ij}; for Bayesian models, the
            parameter value; for the Ginzburg-Landau potential on a lattice, the entire array of superconducting phase.
        charges : optional
            All the charges needed to calculate the gradient.

        Returns
        -------
        float
            The observations (of the target and momentum distributions) generated by the Markov chain.
     """
        support_variable_dimension = len(np.atleast_1d(support_variable))  # todo np.atleast_1d(), c'est quoi ?
        accepted = 0.
        divergences = 0
        support_variable_store = np.empty((support_variable_dimension, self._number_of_observations))
        momentum_store = np.empty((support_variable_dimension, self._number_of_observations))

        for i in range(self._number_of_observations):
            momentum = self._kinetic_energy_instance.momentum_observation(len(support_variable))
            if self._random_number_of_integration_steps:
                number_of_integration_steps = 1 + np.random.randint(self._max_number_of_integration_steps)
            else:
                number_of_integration_steps = self._max_number_of_integration_steps
            support_variable_candidate, momentum_candidate = self._integrator_instance.flow(
                momentum, support_variable, number_of_integration_steps, self._step_size, charges=None)
            if self._use_metropolis_accept_reject:
                delta_hamiltonian = (self._potential_instance.potential(support_variable, charges=charges) +
                                     self._kinetic_energy_instance.kinetic_energy(momentum) -
                                     self._potential_instance.potential(support_variable_candidate, charges=charges) -
                                     self._kinetic_energy_instance.kinetic_energy(momentum_candidate))
                if delta_hamiltonian < - 1000.0:  # todo positive divergences???
                    divergences += 1  # count numerical divergences
                upsilon = np.random.uniform(0, 1)
                if np.log(upsilon) < delta_hamiltonian:  # todo delta_hamiltonian < 0.0???
                    support_variable = support_variable_candidate
                    momentum = momentum_candidate
                    accepted += 1
            else:
                support_variable = support_variable_candidate
                momentum = momentum_candidate
            support_variable_store[:, i] = support_variable
            momentum_store[:, i] = momentum

            # Adapt step-size if in equilibration phase
            if i < self._number_of_equilibration_iterations and (i + 1) % 100 == 0:
                accept_rate = accepted / 100.0
                if accept_rate > 0.8:
                    self._step_size *= 1.1
                elif accept_rate < 0.6:
                    self._step_size *= 0.9
                accepted = 0

        accept_rate = accepted / float(self._number_of_observations - self._number_of_equilibration_iterations)
        print("Acceptance rate: %f" % accept_rate)
        print("LF Steps: %d, Step-size: %.3f" % (self._max_number_of_integration_steps, self._step_size))
        print("Numerical divergences: %d" % divergences)

        # bind initial values with MCMC output
        x_return = np.empty((support_variable_dimension, self._number_of_observations + 1))
        p_return = np.empty((support_variable_dimension, self._number_of_observations + 1))
        x_return[:, 0] = support_variable
        x_return[:, 1:] = support_variable_store
        p_return[:, 0] = np.zeros(support_variable_dimension)
        p_return[:, 1:] = momentum_store

        return {'x': x_return, 'p': p_return}
